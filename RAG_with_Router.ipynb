{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAsj88npPdRu"
      },
      "outputs": [],
      "source": [
        "!pip install openai\n",
        "!pip install sentence-transformers\n",
        "!pip install langchain pypdf langchain-openai llama-index llama-index-question-gen-openai pypdf #tiktoken chromadb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9SSNF_3NdKY"
      },
      "outputs": [],
      "source": [
        "!pip install nest-asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlNnTJmFN6k9"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7t2rkYZEpYE"
      },
      "outputs": [],
      "source": [
        "!wget https://www.goldmansachs.com/pdfs/insights/podcasts/episodes/ai-tom-acemoglu-covello/transcript.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJOpJn7ROq0h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSZ_DMYjh4dE"
      },
      "source": [
        "# Routing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_iyRDzlr1kv"
      },
      "source": [
        "# Routing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieU7KNP5r1kw"
      },
      "outputs": [],
      "source": [
        "# Import necessary classes from the llama_index package\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, SummaryIndex\n",
        "from llama_index.core import Settings\n",
        "\n",
        "# Read documents from the specified directory and load a specific document, \"report.pdf\".\n",
        "documents = SimpleDirectoryReader(\"./\").load_data(\"transcript.pdf\")\n",
        "\n",
        "# initialize settings (set chunk size)\n",
        "Settings.chunk_size = 1024\n",
        "nodes = Settings.node_parser.get_nodes_from_documents(documents)\n",
        "\n",
        "# Create a VectorStoreIndex object from the documents. This will involve processing the documents\n",
        "# and creating a vector representation for each of them, suitable for semantic searching.\n",
        "summary_index = SummaryIndex(nodes)\n",
        "vector_index = VectorStoreIndex(nodes)\n",
        "\n",
        "summary_query_engine = summary_index.as_query_engine(\n",
        "    response_mode=\"tree_summarize\",\n",
        "    use_async=True,\n",
        ")\n",
        "vector_query_engine = vector_index.as_query_engine()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xStWN3Mfr1kw"
      },
      "outputs": [],
      "source": [
        "from llama_index.core.query_engine import RouterQueryEngine\n",
        "from llama_index.core.selectors import PydanticSingleSelector\n",
        "from llama_index.core.tools import QueryEngineTool\n",
        "\n",
        "\n",
        "summary_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=summary_query_engine,\n",
        "    description=\"Useful for summarization questions related to the data source\",\n",
        ")\n",
        "vector_tool = QueryEngineTool.from_defaults(\n",
        "    query_engine=vector_query_engine,\n",
        "    description=\"Useful for retrieving specific context related to the data source\",\n",
        ")\n",
        "\n",
        "query_engine = RouterQueryEngine(\n",
        "    selector=PydanticSingleSelector.from_defaults(),\n",
        "    query_engine_tools=[\n",
        "        summary_tool,\n",
        "        vector_tool,\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udXxrR7Qr1kw"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"Is AI overhyped?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9Oqs031r1ky"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(\"Summarize the document in 4 bullet points\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcdIt8vUiU5b"
      },
      "source": [
        "# Sub Question Query Engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmmfVrOfiWu8"
      },
      "outputs": [],
      "source": [
        "# Import necessary classes and modules from llama_index.core and llama_index.core.tools\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
        "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
        "from llama_index.core import Settings\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Load the data from a PDF file located in the current directory using SimpleDirectoryReader\n",
        "# The load_data method reads the content of the file \"transcript.pdf\"\n",
        "documents = SimpleDirectoryReader(\"./\").load_data(\"transcript.pdf\")\n",
        "\n",
        "# Build a VectorStoreIndex from the loaded documents\n",
        "# This index will allow efficient querying of the document content\n",
        "vector_query_engine = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    use_async=True,  # Enable asynchronous processing for faster performance\n",
        ").as_query_engine()  # Convert the index to a query engine\n",
        "\n",
        "# Define a list of query engine tools, each with its own metadata\n",
        "# This setup is necessary for the SubQuestionQueryEngine\n",
        "query_engine_tools = [\n",
        "    QueryEngineTool(\n",
        "        query_engine=vector_query_engine,  # Use the vector_query_engine built above\n",
        "        metadata=ToolMetadata(\n",
        "            name=\"documents\",  # Name of the tool\n",
        "            description=\"Report\",  # Description of the tool\n",
        "        ),\n",
        "    ),\n",
        "]\n",
        "\n",
        "# Create an instance of SubQuestionQueryEngine using the default settings\n",
        "# This query engine can handle sub-questions and use the provided tools for querying\n",
        "query_engine = SubQuestionQueryEngine.from_defaults(\n",
        "    query_engine_tools=query_engine_tools,  # Provide the list of query engine tools\n",
        "    use_async=True,  # Enable asynchronous processing\n",
        ")\n",
        "\n",
        "# Query the SubQuestionQueryEngine with the question \"Is AI overhyped?\"\n",
        "response = query_engine.query(\n",
        "    \"Is AI overhyped?\"\n",
        ")\n",
        "\n",
        "# Print the response from the query\n",
        "print(response.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HHWIyw3YSNM"
      },
      "outputs": [],
      "source": [
        "display(Markdown(response.response))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6H6aP4WUSp21"
      },
      "source": [
        "# Calling OpenAI AssistantAPI (Code interpreter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "173K-wLCSo8S"
      },
      "outputs": [],
      "source": [
        "from llama_index.agent.openai import OpenAIAssistantAgent\n",
        "\n",
        "agent = OpenAIAssistantAgent.from_new(\n",
        "    name=\"Python agent\",\n",
        "    openai_tools=[{\"type\": \"code_interpreter\"}],\n",
        "    instructions=\"You are an expert at writing python code to solve problems.\",\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "response = agent.chat(\n",
        "    \"\"\"Generate code to answer the following question:\n",
        "    How much is the us population likely to grow to by 2030?\n",
        "    Return and answer and the code used.\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjnQJnqVeiNl"
      },
      "outputs": [],
      "source": [
        "print(str(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhkjZdTjYaDF"
      },
      "outputs": [],
      "source": [
        "display(Markdown(response.response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRMHakt8jFw5"
      },
      "outputs": [],
      "source": [
        "population_2023 = 332_000_000  # 332 million\n",
        "\n",
        "# Assumed average annual growth rate\n",
        "annual_growth_rate = 0.7 / 100  # 0.7% growth rate\n",
        "\n",
        "# Number of years from 2023 to 2030\n",
        "years = 2030 - 2023\n",
        "\n",
        "# Calculate the projected population for 2030\n",
        "population_2030 = population_2023 * ((1 + annual_growth_rate) ** years)\n",
        "population_2030"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVPyM_k9Scio"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}