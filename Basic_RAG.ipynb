{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jT6Yd3IqPbwg"
      },
      "outputs": [],
      "source": [
        "!pip install pypdf llama_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7t2rkYZEpYE"
      },
      "outputs": [],
      "source": [
        "!wget https://privatewealth.goldmansachs.com/outlook/2025-isg-outlook.pdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJOpJn7ROq0h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "J0qSFeK-5sK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary classes from the llama_index package\n",
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader"
      ],
      "metadata": {
        "id": "p-1hHMwH2deX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivg6ru6pOE4v"
      },
      "outputs": [],
      "source": [
        "# Read documents from the specified directory and load a specific document, \"report.pdf\".\n",
        "documents = SimpleDirectoryReader(\"./\").load_data(\"2025-isg-outlook.pdf\")\n",
        "\n",
        "# Create a VectorStoreIndex object from the documents. This will involve processing the documents\n",
        "# and creating a vector representation for each of them, suitable for semantic searching.\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Convert the VectorStoreIndex object into a query engine. This query engine can be used to\n",
        "# perform semantic searches on the index, matching natural language queries to the most relevant\n",
        "# documents in the index.\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Use the query engine to search for documents that are relevant to the query\n",
        "# from the indexed documents based on the semantic understanding of the query.\n",
        "response = query_engine.query(\"What is the 2025 outlook for US GDP?\")\n",
        "\n",
        "# Print the response obtained from the query. This will display the result of the semantic search,\n",
        "# showing the information or documents that best match the query about the 2025 outlook.\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaKXksKrUl5n"
      },
      "source": [
        "# Adding memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTqJ31DI5ngW"
      },
      "outputs": [],
      "source": [
        "chat_engine = index.as_chat_engine(chat_mode=\"simple\", verbose=True)\n",
        "response = chat_engine.chat(\"Hi there\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAwmJolMFodz"
      },
      "outputs": [],
      "source": [
        "response = chat_engine.chat(\"What is the liklihood of recession in 2025?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHiuj5FXGQV2"
      },
      "outputs": [],
      "source": [
        "response = chat_engine.chat(\"Can you expand on that\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUOnFt5rGVtl"
      },
      "outputs": [],
      "source": [
        "chat_engine.chat_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ArFAx74T8EQ"
      },
      "source": [
        "# Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qVBB8_VUMCE"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import Settings\n",
        "\n",
        "\n",
        "Settings.chunk_size = 1000\n",
        "Settings.chunk_overlap = 50\n",
        "\n",
        "# Create a VectorStoreIndex object from the documents. This will involve processing the documents\n",
        "# and creating a vector representation for each of them, suitable for semantic searching.\n",
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Convert the VectorStoreIndex object into a query engine. This query engine can be used to\n",
        "# perform semantic searches on the index, matching natural language queries to the most relevant\n",
        "# documents in the index.\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Use the query engine to search for documents that are relevant to the query\n",
        "# from the indexed documents based on the semantic understanding of the query.\n",
        "response = query_engine.query(\"What is the 2025 outlook for US GDP?\")\n",
        "\n",
        "# Print the response obtained from the query. This will display the result of the semantic search,\n",
        "# showing the information or documents that best match the query about the 2025 outlook.\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyaNMLEFT-5X"
      },
      "source": [
        "# Vector DB"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "onSUkcpVxb7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX5TjvzrVJQY"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index-vector-stores-faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRyTUY7WU1YC"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "\n",
        "# dimensions of text-ada-embedding-002\n",
        "d = 1536\n",
        "faiss_index = faiss.IndexFlatL2(d)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import (\n",
        "    SimpleDirectoryReader,\n",
        "    load_index_from_storage,\n",
        "    VectorStoreIndex,\n",
        "    StorageContext,\n",
        ")\n",
        "from llama_index.vector_stores.faiss import FaissVectorStore"
      ],
      "metadata": {
        "id": "U9T1HBIZxJBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSjxOxGSU8w-"
      },
      "outputs": [],
      "source": [
        "documents = SimpleDirectoryReader(\"./\").load_data(\"2025-isg-outlook.pdf\")\n",
        "vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents, storage_context=storage_context\n",
        ")\n",
        "query_engine = index.as_query_engine()\n",
        "response = query_engine.query(\"What is the 2025 outlook for US GDP?\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save index to disk\n",
        "index.storage_context.persist()"
      ],
      "metadata": {
        "id": "lo5SFVP_yDFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load index from disk\n",
        "vector_store = FaissVectorStore.from_persist_dir(\"./storage\")\n",
        "storage_context = StorageContext.from_defaults(\n",
        "    vector_store=vector_store, persist_dir=\"./storage\"\n",
        ")\n",
        "index = load_index_from_storage(storage_context=storage_context)"
      ],
      "metadata": {
        "id": "Px0NcIA7yENb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "minDGQY_IADb"
      },
      "source": [
        "# Wikipedia\n",
        "\n",
        "Other readers: https://llamahub.ai/?tab=readers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-readers-wikipedia"
      ],
      "metadata": {
        "id": "L13stav40lnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "id": "v_Sr7j_z1zoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRSjb1mi5Sc2"
      },
      "outputs": [],
      "source": [
        "from llama_index.readers.wikipedia import WikipediaReader\n",
        "\n",
        "# Initialize WikipediaReader\n",
        "reader = WikipediaReader()\n",
        "\n",
        "# Load data from Wikipedia\n",
        "documents = reader.load_data(pages=[\"List of most-visited websites\", \"ChatGPT\", \"Freemium\"])\n",
        "\n",
        "index = VectorStoreIndex.from_documents(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjRfv_LyIJqW"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_documents(documents)\n",
        "\n",
        "# Convert the VectorStoreIndex object into a query engine. This query engine can be used to\n",
        "# perform semantic searches on the index, matching natural language queries to the most relevant\n",
        "# documents in the index.\n",
        "query_engine = index.as_query_engine()\n",
        "\n",
        "# Use the query engine to search for documents that are relevant to the query\n",
        "# from the indexed documents based on the semantic understanding of the query.\n",
        "response = query_engine.query(\"Top 10 most visited websites in the Internet\")\n",
        "\n",
        "# Print the response obtained from the query. This will display the result of the semantic search,\n",
        "# showing the information or documents that best match the query about the 2025 outlook.\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDdJC3U0INiE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}